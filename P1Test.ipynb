{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import math\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dict = {}  #Term Frequency\n",
    "df_dict = defaultdict(int)\n",
    "normalized_tf_idf = {}\n",
    "N=0 # Total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpusroot = './US_Inaugural_Addresses'\n",
    "for filename in os.listdir(corpusroot):\n",
    "    if filename.endswith('.txt'):\n",
    "        file = open(os.path.join(corpusroot, filename), \"r\", encoding='windows-1252')\n",
    "        doc = file.read()\n",
    "        file.close() \n",
    "        doc = doc.lower()\n",
    "        \n",
    "        # Tokenize the document\n",
    "        tokenize = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "        tokens = tokenize.tokenize(doc)\n",
    "        \n",
    "        # print(f\"before: {tokens}\")\n",
    "        \n",
    "        #remvoving the stop words\n",
    "        stop_words_list = stopwords.words('english')\n",
    "        tokens_without_stopwords = [t for t in tokens if t not in stop_words_list]\n",
    "        \n",
    "        # print(f\"after: {tokens_without_stopwords}\")\n",
    "        \n",
    "        # Stemming on obtained tokens\n",
    "        stemmer = PorterStemmer()\n",
    "        final_tokens = [stemmer.stem(token) for token in tokens_without_stopwords]\n",
    "        \n",
    "        # Term frequency for document\n",
    "        tf_dict[filename] = Counter(final_tokens)\n",
    "        \n",
    "        # Document Frequency for unique tokens\n",
    "        unique_tokens = set(final_tokens)  \n",
    "        for token in unique_tokens:\n",
    "            df_dict[token] += 1\n",
    "        \n",
    "        N+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_idf(token):\n",
    "    return math.log10(N / df_dict[token]) if df_dict[token] > 0 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getidf(token):\n",
    "    stemmed_token = stemmer.stem(token)\n",
    "    return calculate_idf(stemmed_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tf-idf value \n",
    "def get_tf_idf_weight(filename,token):\n",
    "    if(tf_dict[filename][token])!=0:\n",
    "        return (1+math.log10(tf_dict[filename][token]))*calculate_idf(token)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize document vector\n",
    "def normalize(weights):\n",
    "    length = math.sqrt(sum(weight ** 2 for weight in weights.values()))\n",
    "    return {token: weight / length for token, weight in weights.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build normalized TF-IDF vectors\n",
    "for filename in tf_dict:\n",
    "    tf_idf_weight = {token: get_tf_idf_weight(filename, token) for token in tf_dict[filename]}\n",
    "    normalized_tf_idf[filename] = normalize(tf_idf_weight)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve normalized TF-IDF weight for a term in a document\n",
    "def return_weight(filename, token):\n",
    "    return normalized_tf_idf[filename].get(token, 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of postings list\n",
    "list_sorted_tf_idf = {}\n",
    "\n",
    "# list for each token (sorted by TF-IDF weights)\n",
    "for filename, tfidf_vector in normalized_tf_idf.items():\n",
    "    for token, weight in tfidf_vector.items():\n",
    "        if token not in list_sorted_tf_idf:\n",
    "            list_sorted_tf_idf[token] = []\n",
    "        list_sorted_tf_idf[token].append((filename, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in list_sorted_tf_idf:\n",
    "    list_sorted_tf_idf[token].sort(key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(qstring):\n",
    "    query_tokens = qstring.lower().split()\n",
    "    stemmed_query_tokens = [stemmer.stem(token) for token in query_tokens]\n",
    "    \n",
    "    query_tf = {}\n",
    "    length = 0\n",
    "    \n",
    "    # Calculate query term frequency and magnitute\n",
    "    for token in stemmed_query_tokens:\n",
    "        if token not in query_tf:\n",
    "            query_tf[token] = 1 + math.log10(stemmed_query_tokens.count(token))\n",
    "        length += query_tf[token] ** 2\n",
    "    \n",
    "    query_magnitude = math.sqrt(length)\n",
    "    \n",
    "    actual_scores = Counter() #this stores the cosine similarity of complete matches\n",
    "    upper_bound_scores = Counter() #this stores cosine similarity value of partial match and upper bound\n",
    "\n",
    "    # Cosine similarity\n",
    "    for token in stemmed_query_tokens:\n",
    "        if token in list_sorted_tf_idf:\n",
    "            top_10_postings = list_sorted_tf_idf[token][:10]  # top-10 elements\n",
    "            # print(f\"Token={token} Posting= {top_10_postings}\")\n",
    "            \n",
    "            # 10th weight is the upper-bound weight \n",
    "            upper_bound_weight = top_10_postings[-1][1] if len(top_10_postings) == 10 else 0\n",
    "            \n",
    "            for doc, weight in top_10_postings:\n",
    "                actual_scores[doc] += query_tf[token] * weight / query_magnitude\n",
    "            \n",
    "            # Document not in top-10 get the weight as upper-bound)\n",
    "            for doc in normalized_tf_idf:\n",
    "                if doc not in actual_scores:\n",
    "                    upper_bound_scores[doc] += query_tf[token] * upper_bound_weight / query_magnitude\n",
    "        else:\n",
    "            # If no document contains any token in the query return None\n",
    "            return (\"None\", 0)\n",
    "    \n",
    "    # If actual_scores is empty, return None\n",
    "    if not actual_scores:\n",
    "        return (\"None\", 0)\n",
    "    \n",
    "    # Merge actual and upper-bound scores\n",
    "    for doc in normalized_tf_idf:\n",
    "        if doc not in actual_scores:\n",
    "            actual_scores[doc] = upper_bound_scores[doc]\n",
    "    \n",
    "    # Finding the document with the highest actual score\n",
    "    best_doc = max(actual_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "# If the best document is in upper-bound scores, fetch more elements\n",
    "    if best_doc[0] in upper_bound_scores:\n",
    "        return (\"fetch more\", 0)\n",
    "    else:\n",
    "    # If it's not, return the best document and its score\n",
    "        return best_doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweight(filename, token):\n",
    "    stemmed_token = stemmer.stem(token)\n",
    "    return return_weight(filename, stemmed_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698970004336\n",
      "0.187086643357\n",
      "0.057991946978\n",
      "0.139661993429\n",
      "0.033858267261\n",
      "--------------\n",
      "0.006537500538\n",
      "0.008278952636\n",
      "0.002917313392\n",
      "0.028125012259\n",
      "0.077484582245\n",
      "--------------\n",
      "(21_grant_1869.txt, 0.016899649589)\n",
      "(20_lincoln_1865.txt, 0.126707853305)\n",
      "(07_madison_1813.txt, 0.087561769740)\n",
      "(15_polk_1845.txt, 0.073076294576)\n",
      "(22_grant_1873.txt, 0.010700041427)\n"
     ]
    }
   ],
   "source": [
    "print(\"%.12f\" % getidf('democracy'))\n",
    "print(\"%.12f\" % getidf('foreign'))\n",
    "print(\"%.12f\" % getidf('states'))\n",
    "print(\"%.12f\" % getidf('honor'))\n",
    "print(\"%.12f\" % getidf('great'))\n",
    "print(\"--------------\")\n",
    "print(\"%.12f\" % getweight('19_lincoln_1861.txt','constitution'))\n",
    "print(\"%.12f\" % getweight('23_hayes_1877.txt','public'))\n",
    "print(\"%.12f\" % getweight('25_cleveland_1885.txt','citizen'))\n",
    "print(\"%.12f\" % getweight('09_monroe_1821.txt','revenue'))\n",
    "print(\"%.12f\" % getweight('37_roosevelt_franklin_1933.txt','leadership'))\n",
    "print(\"--------------\")\n",
    "print(\"(%s, %.12f)\" % query(\"states laws\"))\n",
    "print(\"(%s, %.12f)\" % query(\"war offenses\"))\n",
    "print(\"(%s, %.12f)\" % query(\"british war\"))\n",
    "print(\"(%s, %.12f)\" % query(\"texas government\"))\n",
    "print(\"(%s, %.12f)\" % query(\"world civilization\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698970004336\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
